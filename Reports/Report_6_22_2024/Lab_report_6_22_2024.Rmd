---
title: "HCD Simulations Write Up"
author: "Audrey Fu Lab"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2: default
  bookdown::pdf_document2: default
number_sections: false
always_allow_html: true
header-includes:
  - \usepackage{float}
  - \usepackage{multirow}
  - \usepackage{lastpage}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
bibliography: C:/Users/Bruin/Desktop/Research Assistantship/Thesis Proposal Defense/proposal_references.bib
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=10, fig.pos = "H")
#knitr::opts_chunk$set(list(echo = FALSE, eval = TALSE))
library(kableExtra)
library(knitr)
library(latex2exp)
library(gridExtra)
library(ggpubr)
library(ggthemes)
library(plyr)
basepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/'
figpath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Reports/Report_5_13_2024/'

library(reticulate)
use_condaenv('C:\\ProgramData\\anaconda3/python.exe')
library(ggpubr)
# tab = read.csv(paste0(basepath, 'DATA/Toy_examples/Intermediate_examples/Results/MASTER_results_old_5_2_2024.csv'))


compile.to.pdf = FALSE

```

# Data Simulation {-}

### Simulating networks {-}
We adopt a top-down approach to simulate hierarchical networks, considering various simulation parameters such as graph sparsity, noise, and the architecture of the super-level graph(s), including small-world, scale-free, and random graph networks [@watts1998collective; @barabasi2003scale].

Our simulations focus on basic hierarchies comprising one or two hierarchical layers. Two-layer networks mirror classical community detection on graphs, where our aim is to recover the true community labels from a given graph. Meanwhile, three-layer networks present a more intricate scenario, where the bottom layer of the hierarchy contains two levels of community structure. Here, the top level corresponds to the nodes at the uppermost layer of the hierarchy, and the middle level consists of communities nested within the top-level communities. The objective with these networks is to identify both sets of community partitions.
	
In each hierarchy, for fully connected networks, we initiate by simulating $n_{\text{top}}$ top-level nodes, adhering to a directed small-world, random graph, or scale-free network architecture [@watts1998collective; @barabasi2003scale]. In cases where the network is disconnected, we simply simulate $n_{\text{top}}$ disconnected nodes. For networks with three hierarchical layers, we then generate a subnetwork of $n_{\text{middle}}$ nodes from each top-layer node, adhering to the network structure utilized at the top level. If the network is fully connected, we apply a probability $p_\text{between}$ to the nodes from different top-level communities being connected. 

The final step in all hierarchies is to generate the nodes in the observed (bottom) layer of the hierarchy. For each top-layer or middle-layer node, we generate a subnetwork of $n_{\text{bottom}}$ nodes under the same subnetwork structure as the previous layers, and we apply a probability $p_\text{between}$ for nodes from different communities to share an edge.

### Simulating gene expression {-}

Once we simulate a hierarchical graph, we utilize this hierarchy to generate the node-feature matrix, which depicts the expression of $N$ genes across $p$ samples. Here, $N$ denotes the number of nodes in the observed (bottom) layer of the hierarchy, and its range is governed by $a^{\ell+1}<N<a\times b^\ell$, where $\ell$ signifies the number of hierarchical layers.

We simulate the node-feature matrix using the topological order the observed level graph. We start by generating the features of nodes that have no parental input. We refer to these nodes as *origin* nodes. All origin nodes are simulated from a normal distribution with mean $0$ and standard deviation $\sigma$. All other nodes are simulated from a normal distribution centered at the mean of their parent nodes and with standard deviation $\sigma$. 

# Hierarchical Commuity Detection (HCD) Overview {-}

Our HCD method consists of two primary components:

  1. A graph autoencoder based on the architecture proposed by @salehi2019graph which utilizes graph attention layers such as those first indroduced by @velivckovic2017graph (See most recent version of pseudocode for details). In our applications, we incorporate multi-head attention in all encoder and decoder layers to expand model learning capacity. The graph autoencoder module takes a set of node attributes and an adjacency matrix defining the relationships between the node as input and learns a low dimensional embedding of the network and attributes. This embedding is then used to reconstruct the node attributes and adjacency matrix under a separate loss function for each.  
  
  2. The second component of HCD takes the embeddings generated by the autoencoder and applies a multilevel community detection process. This module is composed of $l$ fully connected layers, each representing a level in the hierarchy. Each layer's goal is to group the $k_{i-1}$ nodes from the previous level into $k_i$ communities. The number of layers in the hierarchy and the number of communities at each level are predefined parameters that need to be determined through other methods. 
  In our applications, we use the simulation truth for each parameter so that the communitiy detection module consists of $2$ layers where the first layer assigns the nodes at the bottom layer to the true number of communities at the middle layer. The second community detection layer assigns the nodes in the middle layer to communities in the top layer. 

# Datasets {-}
	
We consider three sets of hierarchical networks which represent varying difficulty levels for inference:
	

  1. **Complex networks** - used for final simulation assessment - **Table** \@ref(tab:tab1) - \@ref(tab:tab3) .
  
  2. **Intermediate networks** - used for investigative model tuning and performance assessment - **Table** \@ref(tab:tab4) .
  
  3. **Simple networks** - used for code implementation and debugging - **Table** \@ref(tab:tab5).

# Application to Intermediate Networks {-}


A comprehensive overview of the intermediate networks is presented in **Table** \@ref(tab:tab4). These networks are structured as three-layered systems, each characterized by small-world, scale-free, or random graph architectures. In contrast to the more intricate networks featured in the **Complex Networks** dataset, the intermediate networks exhibit a comparatively simpler configuration. Specifically, each network comprises $5$ super layer nodes, $15$ middle layer nodes, and $300$ bottom layer nodes. Our primary focus in utilizing this dataset is to examine the performance of the Hierarchical Community Detection (HCD) method when applied to three-layer networks. The smaller scale of these networks facilitates a more in-depth analysis of the detected communities within the middle and upper layers of their hierarchical structures. 

We apply the HCD method to each network separately using three options for the input graph corresponding to the nodes at the observed layer of the hierarchy:

  * The input graph is the true graph
  
  *  The input graph is the correlation matrix of the simulated gene expression
  
  *  The input graph is the correlation matrix of the simulated gene expression wherein correlations weaker than 0.2 are disregarded and set to zero
  
  *  The input graph is the correlation matrix of the simulated gene expression wherein correlations weaker than 0.5 are disregarded and set to zero
  
  *  The input graph is the correlation matrix of the simulated gene expression wherein correlations weaker than 0.7 are disregarded and set to zero
  

We also explore various combinations of weighting the loss function across each of the aforementioned input graphs. In all cases, we ensure that the predicted number of communities in the middle or top levels of the hierarchy aligns with the ground truth of the simulation.

### Evaluating performance {-}

We evaluate the performance of our HCD method using three graph-based clustering metrics:

   1. **homogeneity** evaluates the degree to which each predicted community contains only data points from a single true community, indicating how well the algorithm avoids mixing different groups. Thus, homogenity tends to be high if resolved communities contain only members of the same true community.
   
   2. **completeness** assesses the extent to which all data points that belong to the same true
community are correctly assigned to a single predicted community. Thus completeness is always high if all members of the same true communities end up in the same resolved community even if several true communities are allocated together. 
   
   3. **NMI** is a weighted average of the previous two metrics. 

For each simulation, we configure the number of communities in the middle and upper layers of the hierarchy to match the true count in each layer. Then, we evaluate the community predictions of the Hierarchical Community Detection (HCD) algorithm at these levels against the actual communities using three metrics. As a baseline, we employ the Louvain method, which utilizes hierarchical graph partitioning to maximize modularity, resulting in a single set of resolved communities. These resolved communities may align with the middle, upper, or a combination of both layers in the true hierarchy. Thus, we compute the performance metrics of the communities identified by the Louvain method against the true communities at both the upper and middle levels of the hierarchy.


### Examples {-}



### Issues With Similating Graphs and Data Generation {-}

Our previous reports have outlined that the HCD model can accurately capture at least one level in the hierarchy as well as the Louvain method and the HCD method showed improved capability when the input graph was an estimate of the true adjacency matrix. However, only in one of the examples (**Example 12** Report 6/5/2024) we investigated did HCD accurately predict the communities at both the top and middle levels. Moreover, closer investigation of the true graph and data indicates that the data often has clear community structure at one level in the hierarchy. For example, in the 3-layer scale free network, community structure is very apparent at the middle level of the hierarchy but the top level community structure is less obvious in the correlations or adjacency matrix \@ref(fig:example121314fig1) - \@ref(fig:example121314fig2). In our examples dealing with scale free networks, HCD and the Louvain method typically accurately predicted the middle layer with HCD failing to find the community groupings at the top layer (**Figure** 2.9 Report 6/5/2024). 






```{python, echo = F, message = F, warning = F}
# -*- coding: utf-8 -*-
"""
Created on Sun Jun 23 15:21:08 2024

@author: Bruin
"""



import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import argparse
import numpy as np
import networkx as nx
import seaborn as sbn
import matplotlib.pyplot as plt
import sys
import pandas as pd
#sys.path.append('/mnt/ceph/jarredk/scGNN_for_genes/HC-GNN/')
#sys.path.append('/mnt/ceph/jarredk/HGRN_repo/Simulated Hierarchies/')
#sys.path.append('/mnt/ceph/jarredk/HGRN_repo/HGRN_software/')
#sys.path.append('/mnt/ceph/jarredk/scGNN_for_genes/gen_data')
#sys.path.append('C:/Users/Bruin/Documents/GitHub/scGNN_for_genes/gen_data')
#sys.path.append('C:/Users/Bruin/Documents/GitHub/scGNN_for_genes/HC-GNN/')
sys.path.append('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/')
sys.path.append('C:/Users/Bruin/Documents/GitHub/HGRN_repo/HGRN_software/')
from simulation_software.Simulate import simulate_graph
from simulation_software.simulation_utilities import compute_graph_STATs, sort_labels
from model.utilities import get_input_graph
#import os
#os.chdir('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Bethe Hessian Tests/')
import warnings
warnings.filterwarnings('ignore')
parser = argparse.ArgumentParser()
# general
import random as rd
from itertools import product
from tqdm import tqdm
import time
from sklearn.manifold import TSNE
import pickle
from sklearn.decomposition import PCA
from model.model_layer import gaeGAT_layer as GAT
from model.model import GATE, CommClassifer, HCD
from model.train import CustomDataset, batch_data, fit
from simulation_software.simulation_utilities import compute_modularity, post_hoc_embedding, compute_beth_hess_comms
from model.utilities import resort_graph, trace_comms, node_clust_eval, gen_labels_df, LoadData, build_true_graph, get_input_graph, plot_nodes, plot_clust_heatmaps
import seaborn as sbn
import matplotlib.pyplot as plt
from community import community_louvain as cl
from itertools import product, chain
from tqdm import tqdm
import pdb
import ast
import random as rd
from sklearn.manifold import TSNE
import pickle
from sklearn.decomposition import PCA

# rd.seed(123)
# torch.manual_seed(123)



# simulation default arguments
parser.add_argument('--connect', dest='connect', default='disc', type=str)
parser.add_argument('--connect_prob', dest='connect_prob', default='use_baseline', type=str)
parser.add_argument('--toplayer_connect_prob', dest='toplayer_connect_prob', default=0.3, type=float)
parser.add_argument('--top_layer_nodes', dest='top_layer_nodes', default=5, type=int)
parser.add_argument('--subgraph_type', dest='subgraph_type', default='small world', type=str)
parser.add_argument('--subgraph_prob', dest='subgraph_prob', default=0.05, type=float)
parser.add_argument('--nodes_per_super2', dest='nodes_per_super2', default=(3,3), type=tuple)
parser.add_argument('--nodes_per_super3', dest='nodes_per_super3', default=(20,20), type=tuple)
parser.add_argument('--node_degree_middle', dest='node_degree_middle', default=5, type=int)
parser.add_argument('--node_degree_bottom', dest='node_degree_bottom', default=5, type=int)
parser.add_argument('--sample_size',dest='sample_size', default = 500, type=int)
parser.add_argument('--layers',dest='layers', default = 2, type=int)
parser.add_argument('--SD',dest='SD', default = 0.1, type=float)
parser.add_argument('--common_dist', dest='common_dist',default = True, type=bool)
parser.add_argument('--seed_number', dest='seed_number',default = None, type=int)
parser.add_argument('--within_edgeweights', dest='within_edgeweights',default = (0.4, 0.7), type=tuple)
parser.add_argument('--between_edgeweights', dest='between_edgeweights',default = (0, 0.3), type=tuple)
parser.add_argument('--use_weighted_graph', dest='use_weighted_graph',default = False, type=bool)
args = parser.parse_args()
args.connect_prob = 0.01
args.common_dist = False
args.subgraph_prob = 0.1
args.SD = 0.1
args.node_degree_bottom = 3
args.node_degree_middle = 3
args.force_connect = True
args.layers = 3
args.connect = 'disc'
args.subgraph_type = 'small world'
args.use_weighted_graph = False
#args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/Single_sim_dpd_identity/'

args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/'
#args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Reports/Report_4_15_2024/example7_output/'



def run_simulations(args, use_gpu = False):
    
    device = 'cuda:'+str(0) if use_gpu and torch.cuda.is_available() else 'cpu'
    
    print('*********** using DEVICE: {} **************'.format(device))
    
    
    pe, gexp, nodes, edges, nx_all, adj_all, args.savepath, nodelabs, orin = simulate_graph(args)
    
    true_adj_undi = build_true_graph(args.savepath+'.npz')
    
    pdadj = pd.DataFrame(adj_all[-1])
    pdadj.to_csv(args.savepath+'example_adjacency_matrix.csv')


    in_graph05, in_adj05 = get_input_graph(X = pe, 
                                           method = 'Correlation', 
                                           r_cutoff = 0.5)

    pdadj2 = pd.DataFrame(in_adj05)
    pdadj2.to_csv(args.savepath+'example_adjacency_matrix_corrgraph.csv')

    indices_top, indices_mid, labels_df, sorted_true_labels_top, sorted_true_labels_middle = sort_labels(nodelabs)


    #C = F.one_hot(torch.Tensor(sorted_true_labels_top).to(torch.int64)).to(torch.float32)
    #X = torch.Tensor(pe_sorted).requires_grad_()
    
    
    origin_nodes = [i[0] for i in orin]
    if args.layers > 2:
        pe_sorted = pe[indices_mid,:]
        idx = [indices_mid.index(i) for i in origin_nodes]
    else:
        pe_sorted = pe[indices_top,:]
        idx = [indices_mid.index(i) for i in origin_nodes]
        
    fig, ax = plt.subplots(figsize = (12, 10))
    sbn.heatmap(np.corrcoef(pe_sorted))
    plt.show()

    labels = [sorted_true_labels_middle, sorted_true_labels_top]
    TSNE_embed=TSNE(n_components=3, 
                    learning_rate='auto',
                    init='random', 
                    perplexity=3).fit_transform(pe_sorted)
    PCs = PCA(n_components=3).fit_transform(pe_sorted)

    which_labels = ['Middle', 'Top']
    fig, ax = plt.subplots(2,2, figsize = (10,10))
    for i in range(0, len(labels)):
        
        ax[0][i].scatter(TSNE_embed[:,0], TSNE_embed[:,1], 
                         s = 150.0, c = labels[i], cmap = 'plasma')
        ax[0][i].scatter(TSNE_embed[idx,0], TSNE_embed[idx,1],
                     c='red', s = 300, marker = '.')
        ax[0][i].set_xlabel('Dimension 1')
        ax[0][i].set_ylabel('Dimension 2')
        ax[0][i].set_title( 't-SNE '+which_labels[i])
        #adding node labels
        ax[1][i].scatter(PCs[:,0], PCs[:,1], 
                         s = 150.0, c = labels[i], cmap = 'plasma')
        ax[1][i].scatter(PCs[idx,0], PCs[idx,1], 
        c='red', s = 300, marker = '.')
        ax[1][i].set_xlabel('Dimension 1')
        ax[1][i].set_ylabel('Dimension 2')
        ax[1][i].set_title( 'PCA '+which_labels[i])
    
    plt.show()
        
        
        
    fig3d = plt.figure(figsize=(12,10))
    ax3d = plt.axes(projection='3d')
    ax3d.scatter3D(PCs[:,0], PCs[:,1], PCs[:,2], 
                  c=labels[1], cmap='plasma')
    ax3d.scatter3D(PCs[idx,0], PCs[idx,1], PCs[idx,2], 
                 c='red', s = 300, marker = '.',
                 depthshade = False)

    ax3d.set_xlabel('Dimension 1')
    ax3d.set_ylabel('Dimension 2')
    ax3d.set_zlabel('Dimension 3')
    plt.show()



    fig3d = plt.figure(figsize=(12,10))
    ax3d = plt.axes(projection='3d')
    ax3d.scatter3D(TSNE_embed[:,0], TSNE_embed[:,1], TSNE_embed[:,2], 
                  c=labels[0], cmap='plasma')
    ax3d.scatter3D(TSNE_embed[idx,0], TSNE_embed[idx,1], TSNE_embed[idx,2], 
                 c='red', s = 300, marker = '.',
                 depthshade = False)

    ax3d.set_xlabel('Dimension 1')
    ax3d.set_ylabel('Dimension 2')
    ax3d.set_zlabel('Dimension 3')
    plt.show()
    
    
    
run_simulations(args)

```







```{python, echo = F, message = F, warning = F}
# -*- coding: utf-8 -*-
"""
Created on Sun Jun 23 15:21:08 2024

@author: Bruin
"""



import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import argparse
import numpy as np
import networkx as nx
import seaborn as sbn
import matplotlib.pyplot as plt
import sys
import pandas as pd
#sys.path.append('/mnt/ceph/jarredk/scGNN_for_genes/HC-GNN/')
#sys.path.append('/mnt/ceph/jarredk/HGRN_repo/Simulated Hierarchies/')
#sys.path.append('/mnt/ceph/jarredk/HGRN_repo/HGRN_software/')
#sys.path.append('/mnt/ceph/jarredk/scGNN_for_genes/gen_data')
#sys.path.append('C:/Users/Bruin/Documents/GitHub/scGNN_for_genes/gen_data')
#sys.path.append('C:/Users/Bruin/Documents/GitHub/scGNN_for_genes/HC-GNN/')
sys.path.append('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/')
sys.path.append('C:/Users/Bruin/Documents/GitHub/HGRN_repo/HGRN_software/')
from simulation_software.Simulate import simulate_graph
from simulation_software.simulation_utilities import compute_graph_STATs, sort_labels
from model.utilities import get_input_graph
#import os
#os.chdir('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Bethe Hessian Tests/')
import warnings
warnings.filterwarnings('ignore')
parser = argparse.ArgumentParser()
# general
import random as rd
from itertools import product
from tqdm import tqdm
import time
from sklearn.manifold import TSNE
import pickle
from sklearn.decomposition import PCA
from model.model_layer import gaeGAT_layer as GAT
from model.model import GATE, CommClassifer, HCD
from model.train import CustomDataset, batch_data, fit
from simulation_software.simulation_utilities import compute_modularity, post_hoc_embedding, compute_beth_hess_comms
from model.utilities import resort_graph, trace_comms, node_clust_eval, gen_labels_df, LoadData, build_true_graph, get_input_graph, plot_nodes, plot_clust_heatmaps
import seaborn as sbn
import matplotlib.pyplot as plt
from community import community_louvain as cl
from itertools import product, chain
from tqdm import tqdm
import pdb
import ast
import random as rd
from sklearn.manifold import TSNE
import pickle
from sklearn.decomposition import PCA

# rd.seed(123)
# torch.manual_seed(123)



# simulation default arguments
parser.add_argument('--connect', dest='connect', default='disc', type=str)
parser.add_argument('--connect_prob', dest='connect_prob', default='use_baseline', type=str)
parser.add_argument('--toplayer_connect_prob', dest='toplayer_connect_prob', default=0.3, type=float)
parser.add_argument('--top_layer_nodes', dest='top_layer_nodes', default=5, type=int)
parser.add_argument('--subgraph_type', dest='subgraph_type', default='small world', type=str)
parser.add_argument('--subgraph_prob', dest='subgraph_prob', default=0.05, type=float)
parser.add_argument('--nodes_per_super2', dest='nodes_per_super2', default=(3,3), type=tuple)
parser.add_argument('--nodes_per_super3', dest='nodes_per_super3', default=(20,20), type=tuple)
parser.add_argument('--node_degree_middle', dest='node_degree_middle', default=5, type=int)
parser.add_argument('--node_degree_bottom', dest='node_degree_bottom', default=5, type=int)
parser.add_argument('--sample_size',dest='sample_size', default = 500, type=int)
parser.add_argument('--layers',dest='layers', default = 2, type=int)
parser.add_argument('--SD',dest='SD', default = 0.1, type=float)
parser.add_argument('--common_dist', dest='common_dist',default = True, type=bool)
parser.add_argument('--seed_number', dest='seed_number',default = None, type=int)
parser.add_argument('--within_edgeweights', dest='within_edgeweights',default = (0.4, 0.7), type=tuple)
parser.add_argument('--between_edgeweights', dest='between_edgeweights',default = (0, 0.3), type=tuple)
parser.add_argument('--use_weighted_graph', dest='use_weighted_graph',default = False, type=bool)
args = parser.parse_args()
args.connect_prob = 0.05
args.common_dist = False
args.subgraph_prob = 0.1
args.SD = 0.1
args.node_degree_bottom = 15
args.node_degree_middle = 3
args.force_connect = True
args.layers = 3
args.connect = 'disc'
args.subgraph_type = 'small world'
args.use_weighted_graph = False
#args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/Single_sim_dpd_identity/'

args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/'
#args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Reports/Report_4_15_2024/example7_output/'



def run_simulations(args, use_gpu = False):
    
    device = 'cuda:'+str(0) if use_gpu and torch.cuda.is_available() else 'cpu'
    
    print('*********** using DEVICE: {} **************'.format(device))
    
    
    pe, gexp, nodes, edges, nx_all, adj_all, args.savepath, nodelabs, orin = simulate_graph(args)
    
    true_adj_undi = build_true_graph(args.savepath+'.npz')
    
    pdadj = pd.DataFrame(adj_all[-1])
    pdadj.to_csv(args.savepath+'example_adjacency_matrix.csv')


    in_graph05, in_adj05 = get_input_graph(X = pe, 
                                           method = 'Correlation', 
                                           r_cutoff = 0.5)

    pdadj2 = pd.DataFrame(in_adj05)
    pdadj2.to_csv(args.savepath+'example_adjacency_matrix_corrgraph.csv')

    indices_top, indices_mid, labels_df, sorted_true_labels_top, sorted_true_labels_middle = sort_labels(nodelabs)


    #C = F.one_hot(torch.Tensor(sorted_true_labels_top).to(torch.int64)).to(torch.float32)
    #X = torch.Tensor(pe_sorted).requires_grad_()
    
    
    origin_nodes = [i[0] for i in orin]
    if args.layers > 2:
        pe_sorted = pe[indices_mid,:]
        idx = [indices_mid.index(i) for i in origin_nodes]
    else:
        pe_sorted = pe[indices_top,:]
        idx = [indices_mid.index(i) for i in origin_nodes]
        
    fig, ax = plt.subplots(figsize = (12, 10))
    sbn.heatmap(np.corrcoef(pe_sorted))
    plt.show()

    labels = [sorted_true_labels_middle, sorted_true_labels_top]
    TSNE_embed=TSNE(n_components=3, 
                    learning_rate='auto',
                    init='random', 
                    perplexity=3).fit_transform(pe_sorted)
    PCs = PCA(n_components=3).fit_transform(pe_sorted)

    which_labels = ['Middle', 'Top']
    fig, ax = plt.subplots(2,2, figsize = (10,10))
    for i in range(0, len(labels)):
        
        ax[0][i].scatter(TSNE_embed[:,0], TSNE_embed[:,1], 
                         s = 150.0, c = labels[i], cmap = 'plasma')
        ax[0][i].scatter(TSNE_embed[idx,0], TSNE_embed[idx,1],
                     c='red', s = 300, marker = '.')
        ax[0][i].set_xlabel('Dimension 1')
        ax[0][i].set_ylabel('Dimension 2')
        ax[0][i].set_title( 't-SNE '+which_labels[i])
        #adding node labels
        ax[1][i].scatter(PCs[:,0], PCs[:,1], 
                         s = 150.0, c = labels[i], cmap = 'plasma')
        ax[1][i].scatter(PCs[idx,0], PCs[idx,1], 
        c='red', s = 300, marker = '.')
        ax[1][i].set_xlabel('Dimension 1')
        ax[1][i].set_ylabel('Dimension 2')
        ax[1][i].set_title( 'PCA '+which_labels[i])
    
    plt.show()
        
        
        
    fig3d = plt.figure(figsize=(12,10))
    ax3d = plt.axes(projection='3d')
    ax3d.scatter3D(PCs[:,0], PCs[:,1], PCs[:,2], 
                  c=labels[1], cmap='plasma')
    ax3d.scatter3D(PCs[idx,0], PCs[idx,1], PCs[idx,2], 
                 c='red', s = 300, marker = '.',
                 depthshade = False)

    ax3d.set_xlabel('Dimension 1')
    ax3d.set_ylabel('Dimension 2')
    ax3d.set_zlabel('Dimension 3')
    plt.show()



    fig3d = plt.figure(figsize=(12,10))
    ax3d = plt.axes(projection='3d')
    ax3d.scatter3D(TSNE_embed[:,0], TSNE_embed[:,1], TSNE_embed[:,2], 
                  c=labels[0], cmap='plasma')
    ax3d.scatter3D(TSNE_embed[idx,0], TSNE_embed[idx,1], TSNE_embed[idx,2], 
                 c='red', s = 300, marker = '.',
                 depthshade = False)

    ax3d.set_xlabel('Dimension 1')
    ax3d.set_ylabel('Dimension 2')
    ax3d.set_zlabel('Dimension 3')
    plt.show()
    
    
    
run_simulations(args)

```



```{python, echo = F, message = F, warning = F}

# -*- coding: utf-8 -*-
"""
Created on Wed Jan 24 17:54:24 2024

@author: Bruin
"""


import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import argparse
import numpy as np
import networkx as nx
import seaborn as sbn
import matplotlib.pyplot as plt
import sys
import pandas as pd
#sys.path.append('/mnt/ceph/jarredk/scGNN_for_genes/HC-GNN/')
#sys.path.append('/mnt/ceph/jarredk/HGRN_repo/Simulated Hierarchies/')
#sys.path.append('/mnt/ceph/jarredk/HGRN_repo/HGRN_software/')
#sys.path.append('/mnt/ceph/jarredk/scGNN_for_genes/gen_data')
#sys.path.append('C:/Users/Bruin/Documents/GitHub/scGNN_for_genes/gen_data')
#sys.path.append('C:/Users/Bruin/Documents/GitHub/scGNN_for_genes/HC-GNN/')
sys.path.append('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/')
sys.path.append('C:/Users/Bruin/Documents/GitHub/HGRN_repo/HGRN_software/')
from simulation_software.Simulate import simulate_graph
from simulation_software.simulation_utilities import compute_graph_STATs, sort_labels
from model.utilities import get_input_graph
#import os
#os.chdir('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Bethe Hessian Tests/')
import warnings
warnings.filterwarnings('ignore')
parser = argparse.ArgumentParser()
# general
import random as rd
from itertools import product
from tqdm import tqdm
import time
from sklearn.manifold import TSNE
import pickle
from sklearn.decomposition import PCA
from model.model_layer import gaeGAT_layer as GAT
from model.model import GATE, CommClassifer, HCD
from model.train import CustomDataset, batch_data, fit
from simulation_software.simulation_utilities import compute_modularity, post_hoc_embedding, compute_beth_hess_comms
from model.utilities import resort_graph, trace_comms, node_clust_eval, gen_labels_df, LoadData, build_true_graph, get_input_graph, plot_nodes, plot_clust_heatmaps
import seaborn as sbn
import matplotlib.pyplot as plt
from community import community_louvain as cl
from itertools import product, chain
from tqdm import tqdm
import pdb
import ast
import random as rd
from sklearn.manifold import TSNE
import pickle
from sklearn.decomposition import PCA

# rd.seed(123)
# torch.manual_seed(123)
#set seed
#rd.seed(333)






def run_simulations(args, save_results = False, which_net = 0, which_ingraph=0, gam = 1, delt = 1, 
                    lam = 1, learn_rate = 1e-4, epochs = 10, updates = 10, reso = [1,1], 
                    hd = [256, 128, 64], use_true_comms =True, cms = [], 
                    activation = 'LeakyReLU', use_gpu = True, verbose = True,
                    TOAL = False, return_result = ['best_perf_top', 'best_perf_mid'],**kwargs):
    
    device = 'cuda:'+str(0) if use_gpu and torch.cuda.is_available() else 'cpu'
    
    print('*********** using DEVICE: {} **************'.format(device))
    #pathnames and filename conventions
    #mainpath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/'
    #pathnames and filename conventions
    #mainpath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/'
    #loadpath_main = '/mnt/ceph/jarredk/HGRN_repo/Simulated_Hierarchies/'
    #savepath_main ='/mnt/ceph/jarredk/HGRN_repo/Simulated_Hierarchies/test/'
    
    
    pe, gexp, nodes, edges, nx_all, adj_all, args.savepath, nodelabs, orin = simulate_graph(args)
    
    true_adj_undi = build_true_graph(args.savepath+'.npz')
    
    pdadj = pd.DataFrame(adj_all[-1])
    pdadj.to_csv(args.savepath+'example_adjacency_matrix.csv')


    in_graph05, in_adj05 = get_input_graph(X = pe, 
                                           method = 'Correlation', 
                                           r_cutoff = 0.5)

    pdadj2 = pd.DataFrame(in_adj05)
    pdadj2.to_csv(args.savepath+'example_adjacency_matrix_corrgraph.csv')

    indices_top, indices_mid, labels_df, sorted_true_labels_top, sorted_true_labels_middle = sort_labels(nodelabs)


    #C = F.one_hot(torch.Tensor(sorted_true_labels_top).to(torch.int64)).to(torch.float32)
    #X = torch.Tensor(pe_sorted).requires_grad_()
    origin_nodes = [i[0] for i in orin]
    if args.layers > 2:
        pe_sorted = pe[indices_mid,:]
        idx = [indices_mid.index(i) for i in origin_nodes]
    else:
        pe_sorted = pe[indices_top,:]
        idx = [indices_mid.index(i) for i in origin_nodes]

    labels = [sorted_true_labels_middle, sorted_true_labels_top]
    TSNE_embed=TSNE(n_components=3, 
                    learning_rate='auto',
                    init='random', 
                    perplexity=3).fit_transform(pe_sorted)
    PCs = PCA(n_components=3).fit_transform(pe_sorted)

    which_labels = ['Middle', 'Top']
    fig, ax = plt.subplots(2,2, figsize = (10,10))
    for i in range(0, len(labels)):
        
        ax[0][i].scatter(TSNE_embed[:,0], TSNE_embed[:,1], 
                         s = 150.0, c = labels[i], cmap = 'plasma')
        ax[0][i].scatter(TSNE_embed[idx,0], TSNE_embed[idx,1],
                     c='red', s = 300, marker = '.')
        ax[0][i].set_xlabel('Dimension 1')
        ax[0][i].set_ylabel('Dimension 2')
        ax[0][i].set_title( 't-SNE '+which_labels[i])
        #adding node labels
        ax[1][i].scatter(PCs[:,0], PCs[:,1], 
                         s = 150.0, c = labels[i], cmap = 'plasma')
        ax[1][i].scatter(PCs[idx,0], PCs[idx,1], 
        c='red', s = 300, marker = '.')
        ax[1][i].set_xlabel('Dimension 1')
        ax[1][i].set_ylabel('Dimension 2')
        ax[1][i].set_title( 'PCA '+which_labels[i])
        
        
        
        
    fig3d = plt.figure(figsize=(12,10))
    ax3d = plt.axes(projection='3d')
    ax3d.scatter3D(PCs[:,0], PCs[:,1], PCs[:,2], 
                  c=labels[1], cmap='plasma')
    ax3d.scatter3D(PCs[idx,0], PCs[idx,1], PCs[idx,2], 
                 c='red', s = 300, marker = '.',
                 depthshade = False)

    ax3d.set_xlabel('Dimension 1')
    ax3d.set_ylabel('Dimension 2')
    ax3d.set_zlabel('Dimension 3')




    fig3d = plt.figure(figsize=(12,10))
    ax3d = plt.axes(projection='3d')
    ax3d.scatter3D(TSNE_embed[:,0], TSNE_embed[:,1], TSNE_embed[:,2], 
                  c=labels[0], cmap='plasma')
    ax3d.scatter3D(TSNE_embed[idx,0], TSNE_embed[idx,1], TSNE_embed[idx,2], 
                 c='red', s = 300, marker = '.',
                 depthshade = False)

    ax3d.set_xlabel('Dimension 1')
    ax3d.set_ylabel('Dimension 2')
    ax3d.set_zlabel('Dimension 3')
    
    
    #loadpath_main = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/Toy_examples/Intermediate_examples/OLD_1_23_2024/'
    #loadpath_main = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/Single_sim_dpd_identity/'
    #savepath_main = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/Single_sim_dpd_identity/'

    loadpath_main = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/'
    savepath_main = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/'

    #preallocate results table
    res_table = pd.DataFrame(columns = ['Beth_Hessian_Comms',
                                        'Communities_Upper_Limit',
                                        'Max_Modularity',
                                        'Modularity',
                                        'Reconstruction_A',
                                        'Reconstruction_X', 
                                        'Metrics',
                                        'Number_Predicted_Comms',
                                        'Louvain_Modularity',
                                        'Louvain_Metrics',
                                        'Louvain_Predicted_comms'])
    
    tables = [res_table.copy(),res_table.copy(),res_table.copy(),res_table.copy(),
              res_table.copy()]
    
    case = ['A_ingraph_true/','A_corr_no_cutoff/','A_ingraph02/', 
            'A_ingraph05/', 'A_ingraph07/']
    case_nm = ['A_ingraph_true','A_corr_no_cutoff','A_ingraph02', 
               'A_ingraph05', 'A_ingraph07']
    
    #run simulations
    #pdb.set_trace()
    lays = args.layers
    #extract and use true community sizes
    comm_sizes = cms
    #comm_sizes =[40,5]
 
    #combine target labels into list
    print('Read in expression data of dimension = {}'.format(pe.shape))
    if lays == 2:
        target_labels = [sorted_true_labels_top, []]
        #sort nodes in expression table 
        pe_sorted = pe[indices_top,:]
    else:
        target_labels = [sorted_true_labels_top, 
                         sorted_true_labels_middle]
        #sort nodes in expression table 
        pe_sorted = pe[indices_mid,:]
            

    #generate input graphs for correlations r > 0.2, r > 0.5 and r > 0.8
    in_graph02, in_adj02 = get_input_graph(X = pe_sorted, 
                                           method = 'Correlation', 
                                           r_cutoff = 0.2)
    
    in_graph05, in_adj05 = get_input_graph(X = pe_sorted, 
                                           method = 'Correlation', 
                                           r_cutoff = 0.5)
    
    in_graph08, in_adj07 = get_input_graph(X = pe_sorted, 
                                       method = 'Correlation', 
                                       r_cutoff = 0.7)
    
    rmat = np.absolute(np.corrcoef(pe_sorted))
    in_graph_rmat = nx.from_numpy_array(rmat)
    #print network statistics
    
    #nodes and attributes
    nodes = pe.shape[0]
    attrib = pe.shape[1]
    #set up three separate models for true input graph, r > 0.5 input graph, and
    #r > 0.8 input graph scenarios
    print('-'*25+'setting up and fitting models'+'-'*25)

    HCD_model_truth = HCD(nodes, attrib, hidden_dims=hd, 
                          comm_sizes=comm_sizes, attn_act=activation, **kwargs).to(device)
    HCD_model_rmat = HCD(nodes, attrib, hidden_dims=hd,
                         comm_sizes=comm_sizes, attn_act=activation, **kwargs).to(device)
    HCD_model_r02 = HCD(nodes, attrib, hidden_dims=hd, 
                        comm_sizes=comm_sizes, attn_act=activation, **kwargs).to(device)
    HCD_model_r05 = HCD(nodes, attrib, hidden_dims=hd, 
                        comm_sizes=comm_sizes, attn_act=activation, **kwargs).to(device)
    HCD_model_r07 = HCD(nodes, attrib, hidden_dims=hd, 
                        comm_sizes=comm_sizes, attn_act=activation, **kwargs).to(device)
    
    #set attribute and input graph(s) to torch tensors with grad attached
    X = torch.Tensor(pe_sorted).requires_grad_()
    #three input graph scenarios -- add self loops
    A_truth = torch.Tensor(true_adj_undi[:nodes,:nodes]).requires_grad_()+torch.eye(nodes)
    A_rmat = torch.Tensor(rmat).requires_grad_()
    A_r02 = torch.Tensor(in_adj02).requires_grad_()+torch.eye(nodes)
    A_r05 = torch.Tensor(in_adj05).requires_grad_()+torch.eye(nodes)
    A_r07 = torch.Tensor(in_adj07).requires_grad_()+torch.eye(nodes)
    
    #combine input items into lists for iteration
    Mods = [HCD_model_truth, HCD_model_rmat, HCD_model_r02, 
            HCD_model_r05, HCD_model_r07]
    Graphs = [A_truth, A_rmat, A_r02, A_r05, A_r07]
    printing = ['fitting model using true input graph',
                'fitting model using r matrix as input graph',
                'fitting model using r > 0.2 input graph',
                'fitting model using r > 0.5 input graph',
                'fitting model using r > 0.7 input graph']
    
    #preallocate metrics
    metrics = []
    comm_loss = []
    recon_A = []
    recon_X = []
    predicted_comms = []
    louv_mod = []
    louv_num_comms = []
    print('...done')
    #fit the three models
    for i in range(0, 5):
        
        if i == which_ingraph:
            print("*"*80)
            print(printing[i])
            out = fit(Mods[i], X, Graphs[i], 
                      optimizer='Adam', 
                      epochs = epochs, 
                      update_interval=updates, 
                      layer_resolutions=reso,
                      lr = learn_rate, 
                      gamma = gam, 
                      delta = delt, 
                      lamb = lam, 
                      true_labels = target_labels, 
                      verbose=verbose, 
                      save_output=save_results, 
                      turn_off_A_loss= TOAL,
                      output_path=savepath_main, 
                      ns = 25, 
                      fs = 10)
            
            #record best losses and best performances
            #pdb.set_trace()
            total_loss = np.array(out[-5])
            best_loss_idx = total_loss.tolist().index(min(total_loss))
            #perf_top = np.array([list(chain.from_iterable(i)) for i in temp_top])
            #perf_mid = np.array([list(chain.from_iterable(i)) for i in temp_mid])
            
            
            if return_result == 'best_perf_top':
                temp_top = [i[0] for i in out[-1]]
                perf_top = np.array(temp_top)
                best_perf_idx = perf_top[:,2].tolist().index(max(perf_top[:,2]))
                print('Best Performance Top Layer: Epoch = {}, \nHomogeneity = {},\nCompleteness = {}, \nNMI = {}'.format(
                best_perf_idx,      
                perf_top[best_perf_idx, 0],
                perf_top[best_perf_idx, 1],
                perf_top[best_perf_idx, 2]
                ))
            else:
                temp_mid = [i[1] for i in out[-1]]
                perf_mid = np.array(temp_mid)
                best_perf_idx = perf_mid[:,2].tolist().index(max(perf_mid[:,2]))
                print('Best Performance Middle Layer: Epoch = {}, \nHomogeneity = {},\nCompleteness = {}, \nNMI = {}'.format(
                    best_loss_idx,
                    perf_mid[best_perf_idx, 0],
                    perf_mid[best_perf_idx, 1],
                    perf_mid[best_perf_idx, 2]
                    ))
            #update lists
            comm_loss.append(np.round(out[-4][best_loss_idx], 4))
            recon_A.append(np.round(out[-3][best_loss_idx], 4))
            recon_X.append(np.round(out[-2][best_loss_idx], 4))
    
            
            #compute the upper limit of communities, the beth hessian, and max modularity
            upper_limit = torch.sqrt(torch.sum(Graphs[i]-torch.eye(nodes)))
            beth_hessian = compute_beth_hess_comms((Graphs[i]-torch.eye(nodes)).cpu().detach().numpy())
            max_modularity = 1 - (2/upper_limit)
    
            #output assigned labels for all layers
            S_sub, S_layer, S_all = trace_comms(out[6], comm_sizes)
            predicted_comms.append(tuple([len(np.unique(i)) for i in S_layer]))
                
            #get prediction using louvain method
            comms = cl.best_partition(nx.from_numpy_array((Graphs[i]-torch.eye(nodes)).cpu().detach().numpy()))
            louv_mod = cl.modularity(comms, nx.from_numpy_array((Graphs[i]-torch.eye(nodes)).cpu().detach().numpy()))
            #extract cluster labels
            louv_preds = list(comms.values())
            louv_num_comms = len(np.unique(louv_preds))
            #make heatmap for louvain results and get metrics
            fig, ax = plt.subplots()
            #compute performance based on layers
            if lays == 2:
                metrics.append({'Top': tuple(np.round(out[-1][best_perf_idx][0], 4))})
                louv_metrics = {'Top': tuple(np.round(node_clust_eval(target_labels[0], 
                                                             np.array(louv_preds), 
                                                             verbose=False), 4))}
                sbn.heatmap(pd.DataFrame(np.array([louv_preds,  
                                                   target_labels[0].tolist()]).T,
                                         columns = ['Louvain','Truth_Top']),
                            ax = ax)
            else:
                metrics.append({'Top': tuple(np.round(out[-1][best_perf_idx][0], 4)),
                                'Middle': tuple(np.round(out[-1][best_perf_idx][-1], 4))})
                lnm=['Top','Middle']
                louv_metrics = []
                for j in range(0, 2):
                    louv_metrics.append({lnm[j]: tuple(np.round(node_clust_eval(target_labels[j], 
                                                                         np.array(louv_preds), 
                                                                         verbose=False), 4))})
                    sbn.heatmap(pd.DataFrame(np.array([louv_preds, 
                                                       target_labels[1].tolist(), 
                                                       target_labels[0].tolist()]).T,
                                             columns = ['Louvain','Truth_Middle','Truth_Top']),
                                ax = ax)
            #make heatmap for louvain results
            fig.savefig(savepath_main+'Louvain_results.pdf')
            plot_nodes((Graphs[i]-torch.eye(nodes)).cpu().detach().numpy(), 
                       labels = np.array(louv_preds), 
                       path = savepath_main+'Louvain_graph_'+case_nm[i], 
                       node_size = 25, 
                       font_size = 10, 
                       add_labels = True,
                       save = True)
            #update performance table
            row_add = [beth_hessian,
                       np.round(upper_limit.cpu().detach().numpy()),
                       np.round(max_modularity.cpu().detach().numpy(),4),
                       tuple(comm_loss[-1].tolist()), 
                       recon_A[-1], 
                       recon_X[-1],
                       metrics[-1], 
                       predicted_comms[-1],
                       np.round(louv_mod, 4),
                       louv_metrics,
                       louv_num_comms]
            print(row_add)
            print('updating performance statistics...')
            tables[i].loc[0] = row_add
            print('*'*80)
            print(tables[i].loc[0])
            print('*'*80)
            if save_results == True:
                tables[i].to_csv(savepath_main+'Simulation_Results_'+case_nm[i]+'.csv')
                with open(savepath_main+'Simulation_Results_'+'OUTPUT'+'.pkl', 'wb') as f:
                    pickle.dump(out, f)
                
    print('done')
    return out, tables, Graphs, X, target_labels, S_all, S_sub, louv_preds, [best_perf_idx, best_loss_idx]
            


# simulation default arguments
parser.add_argument('--connect', dest='connect', default='disc', type=str)
parser.add_argument('--connect_prob', dest='connect_prob', default='use_baseline', type=str)
parser.add_argument('--toplayer_connect_prob', dest='toplayer_connect_prob', default=0.3, type=float)
parser.add_argument('--top_layer_nodes', dest='top_layer_nodes', default=2, type=int)
parser.add_argument('--subgraph_type', dest='subgraph_type', default='small world', type=str)
parser.add_argument('--subgraph_prob', dest='subgraph_prob', default=0.05, type=float)
parser.add_argument('--nodes_per_super2', dest='nodes_per_super2', default=(3,3), type=tuple)
parser.add_argument('--nodes_per_super3', dest='nodes_per_super3', default=(20,20), type=tuple)
parser.add_argument('--node_degree_middle', dest='node_degree_middle', default=5, type=int)
parser.add_argument('--node_degree_bottom', dest='node_degree_bottom', default=5, type=int)
parser.add_argument('--sample_size',dest='sample_size', default = 500, type=int)
parser.add_argument('--layers',dest='layers', default = 2, type=int)
parser.add_argument('--SD',dest='SD', default = 0.1, type=float)
parser.add_argument('--common_dist', dest='common_dist',default = True, type=bool)
parser.add_argument('--seed_number', dest='seed_number',default = 555, type=int)
parser.add_argument('--within_edgeweights', dest='within_edgeweights',default = (0.5, 0.8), type=tuple)
parser.add_argument('--between_edgeweights', dest='between_edgeweights',default = (0, 0.5), type=tuple)
parser.add_argument('--use_weighted_graph', dest='use_weighted_graph',default = False, type=bool)
args = parser.parse_args()
args.connect_prob = 0.05
args.common_dist = False
args.subgraph_prob = 0.05
args.SD = 0.1
args.force_connect = True
args.layers = 3
args.connect = 'disc'
args.subgraph_type = 'small world'
args.use_weighted_graph = False
args.node_degree_bottom = 15
args.node_degree_middle = 3
#args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/Single_sim_dpd_identity/'

args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples/Results/test/unequal_dists/'
#args.savepath = 'C:/Users/Bruin/Documents/GitHub/HGRN_repo/Reports/Report_4_15_2024/example7_output/'
sp = args.savepath


#cms:
# smw simple data: [true clusts]
# smw complex: []
# sfr simple data: [128, 5] atn: 20 heads, lam = [0.1, 0.1]
ep = 100
wn = 0
wg = 3
saveit = False
#disconnected settings
param_dict = {'dataset': 'unique_dist',
              'epochs': ep, 
              'hidden': [256, 128, 64, 32],
              'gamma': 1e-3,
              'delta': 0,
              'lambda': [1e-4, 1e-5],
              'input_graph': wg,
              'network': wn,
              'learning_rate': 1e-5,
              'true_comms': True,
              'use_attn': True,
              'attn_heads': 20,
              'normalize': True}


out, res, graphs, data, truth, preds, preds_sub, louv_pred, idx = run_simulations(args,
    save_results=saveit,
    which_net=wn,
    which_ingraph=wg,
    reso=[1,1],
    hd=param_dict['hidden'],
    gam = param_dict['gamma'],
    delt = param_dict['delta'],
    lam = param_dict['lambda'],
    learn_rate=param_dict['learning_rate'],
    use_true_comms=param_dict['true_comms'],
    cms=[15, 5],
    epochs = param_dict['epochs'],
    updates = param_dict['epochs'],
    activation = 'Sigmoid',
    TOAL=False,
    use_multi_head=param_dict['use_attn'],
    attn_heads=param_dict['attn_heads'],
    verbose = True,
    return_result = 'best_perf_top',
    normalize_inputs=param_dict['normalize'])

bp, bl = idx



params_df = pd.DataFrame([['Hidden Layer Dimensions: ', str([496, 392, 256, 128, 64, 32])],
                          ['X_loss: ', 1e-4],
                          ['Modularity_loss: ', 1e-4],
                          ['Clustering_loss: ', str([5e-4, 1e-5])],
                          ['Learning_rate: ', 1e-5],
                          ['True Comms: ', str(True)],
                          ['Epochs: ', str(500)],
                          ['Multi Head Attn: ', str(True)],
                          ['Attn. Heads: ', str(30)],
                          ['Normalize_inputs: ',str(True)]],
                          columns=['Parameter','Value'])

params_df.to_csv(sp+'param_settings.csv')


print('Louvain compared to middle')
homo_l2m, comp_l2m, nmi_l2m = node_clust_eval(truth[1], louv_pred)
print('Louvain compared to top')
homo_l2t, comp_l2t, nmi_l2t =node_clust_eval(truth[0], louv_pred)

print('*'*50)
print('*'*50)
print('*'*50)


print('----------Middle preds to middle truth----------')
homo_m2m, comp_m2m, nmi_m2m = node_clust_eval(true_labels=truth[::-1][0], 
                                  pred_labels = out[0][bp][-3][0], verbose=False)
print('Homogeneity = {}, \nCompleteness = {}, \nNMI = {}'.format(
    homo_m2m, comp_m2m, nmi_m2m
    ))


print('----------Middle preds to top truth----------')
homo_m2t, comp_m2t, nmi_m2t = node_clust_eval(true_labels=truth[::-1][1], 
                                  pred_labels = out[0][bp][-3][0], verbose=False)
print('Homogeneity = {}, \nCompleteness = {}, \nNMI = {}'.format(
    homo_m2t, comp_m2t, nmi_m2t
    ))


# print('----------top preds to middle truth----------')
# homo, comp, nmi = node_clust_eval(true_labels=truth[::-1][1], 
#                                   pred_labels = out[0][bp][-2][1], verbose=False)
# print('Homogeneity = {}, \nCompleteness = {}, \nNMI = {}'.format(
#     homo, comp, nmi
#     ))


print('----------top preds to top truth----------')
homo_t2t, comp_t2t, nmi_t2t = node_clust_eval(true_labels=truth[::-1][1], 
                                  pred_labels = out[0][bp][-3][1], verbose=False)
print('Homogeneity = {}, \nCompleteness = {}, \nNMI = {}'.format(
    homo_t2t, comp_t2t, nmi_t2t
    ))

print('*'*50)
print('*'*50)
print('*'*50)


best_iter_metrics = pd.DataFrame([['louvain vs middle']+np.round([homo_l2m, comp_l2m, nmi_l2m],4).tolist(),
                                  ['louvain vs top']+np.round([homo_l2t, comp_l2t, nmi_l2t],4).tolist(),
                                  ['HCD: middle vs middle']+np.round([homo_m2m, comp_m2m, nmi_m2m],4).tolist(),
                                  ['HCD: middle vs top']+np.round([homo_m2t, comp_m2t, nmi_m2t],4).tolist(),
                                  ['HCD: top vs top']+np.round([homo_t2t, comp_t2t, nmi_t2t],4).tolist()],
                                 columns=['Comparison','Homogeneity','Completeness','NMI'])

if saveit:
    best_iter_metrics.to_csv(sp+'best_iteration_toplayer_metrics.csv')


# G = nx.from_numpy_array((out[0][bp][3][0]-torch.eye(data.shape[0])).cpu().detach().numpy())
# templabs = np.arange(0, data.shape[0])
# clust_labels = {list(G.nodes)[k]: templabs.tolist()[k] for k in range(len(truth[1]))}
# nx.draw_networkx(G, node_color = truth[1], 
#                  node_size = 30,
#                  font_size = 8,
#                  with_labels = False,
#                  labels = clust_labels,
#                  cmap = 'plasma')
#Top layer TSNE and PCA
post_hoc_embedding(graph=out[0][bp][3][0]-torch.eye(data.shape[0]), 
                        input_X = data,
                        data=data, 
                        probabilities=out[0][bp][4],
                        size = 150.0,
                        labels = out[0][bp][-3],
                        truth = truth[::-1],
                        fs=10,
                        node_size = 25, 
                        cm = 'plasma',
                        font_size = 10,
                        include_3d_plots = True,
                        save = saveit,
                        path = sp)


plot_clust_heatmaps(A = graphs[wg], 
                    A_pred = out[0][bp][1]-torch.eye(data.shape[0]), 
                    true_labels = truth, 
                    pred_labels = out[0][bp][-3], 
                    layers = 3, 
                    epoch = bp, 
                    save_plot = saveit, 
                    sp = sp+'best_iteration_'+str(bp))










#tsne
TSNE_data=TSNE(n_components=3, 
               learning_rate='auto',
               init='random', 
               perplexity=3).fit_transform(out[0][bp][2][0].detach().numpy())
#pca
PCs = PCA(n_components=3).fit_transform(out[0][bp][2][0].detach().numpy())


fig, (ax1, ax2) = plt.subplots(2,2, figsize = (12,10))
#tsne plot
ax1[0].scatter(TSNE_data[:,0], TSNE_data[:,1], s = 25, c = out[0][bp][-3][1], cmap = 'plasma')
ax1[0].set_xlabel('Dimension 1')
ax1[0].set_ylabel('Dimension 2')
ax1[0].set_title(' t-SNE Embedding Bottleneck (Predicted)')
#adding node labels
    
#PCA plot
ax1[1].scatter(PCs[:,0], PCs[:,1], s = 25, c = out[0][bp][-3][1], cmap = 'plasma')
ax1[1].set_xlabel('Dimension 1')
ax1[1].set_ylabel('Dimension 2')
ax1[1].set_title(' PCA Embedding-Bottleneck_(Predicted)')



x1 = torch.mm(out[0][bp][2][0], out[0][bp][2][1].transpose(0,1))

TSNE_data2=TSNE(n_components=3, 
               learning_rate='auto',
               init='random', 
               perplexity=3).fit_transform(x1.detach().numpy())
#pca
PCs2 = PCA(n_components=3).fit_transform(x1.detach().numpy())

#tsne plot
ax2[0].scatter(TSNE_data2[:,0], TSNE_data2[:,1], s = 25, c = out[0][bp][-3][1], cmap = 'plasma')
ax2[0].set_xlabel('Dimension 1')
ax2[0].set_ylabel('Dimension 2')
ax2[0].set_title(' t-SNE Embedding Comm1-projection (Predicted)')
#adding node labels
    
#PCA plot
ax2[1].scatter(PCs2[:,0], PCs2[:,1], s = 25, c = out[0][bp][-3][1], cmap = 'plasma')
ax2[1].set_xlabel('Dimension 1')
ax2[1].set_ylabel('Dimension 2')
ax2[1].set_title(' PCA Embedding-Comm1-projection (Predicted)')

if saveit == True:
    fig.savefig(sp+'topclusters_plotted_on_embeds.pdf')

```























  

### future points to investigate {-}


# Tables

```{r tab1, eval=T, echo=F, message=F, warning=F}

complex_net_stats = read.csv('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/network_statistics.csv')[,-1]

complex_net_stats$modularity_top = round(complex_net_stats$modularity_top, 3)
complex_net_stats$avg_node_degree_top = round(complex_net_stats$avg_node_degree_top, 3)
complex_net_stats$avg_connect_within_top = round(complex_net_stats$avg_connect_within_top, 3)
complex_net_stats$avg_connect_between_top = round(complex_net_stats$avg_connect_between_top, 3)
complex_net_stats$modularity_middle = round(complex_net_stats$modularity_middle, 3)
complex_net_stats$avg_node_degree_middle = round(complex_net_stats$avg_node_degree_middle, 3)
complex_net_stats$avg_connect_within_middle = round(complex_net_stats$avg_connect_within_middle, 3)
complex_net_stats$avg_connect_between_middle = round(complex_net_stats$avg_connect_between_middle, 3)




if(compile.to.pdf){
  df = rbind.data.frame(c('Subgraph type','Connect. type','Layers','StDev.','Nodes per layer','Edges per layer','Subgraph prob.','Sample size', 'Modularity (top)', 'Avg. node degree top', 'Avg edges within communities (top)','Avg. edges between communities (top)', 'Modularity (middle)', 'Avg. node degree middle', 'Avg edges within communities (middle)', 'Avg edges between communities (middle)'), complex_net_stats)
  kable(t(df[1:9,]), format = 'latex', digits = 2, row.names = F, col.names = c('Value', paste0('Network', c(1:8))),
                                                                                    booktabs = T,
      caption = 'Summary statistics for all small world networks in the complex networks datset')%>%kable_styling(latex_options = c("striped", "hold_postion"),font_size = 10)%>%kable_styling(font_size = 9)%>%column_spec(1, '6em')%>%column_spec(2:9, '4em')
}else{
  
  df = complex_net_stats
  colnames(df) = c('Subgraph type','Connect. type','Layers','StDev.','Nodes per layer','Edges per layer','Subgraph prob.','Sample size', 'Modularity (top)', 'Avg. node degree top', 'Avg edges within communities (top)','Avg. edges between communities (top)', 'Modularity (middle)', 'Avg. node degree middle', 'Avg edges within communities (middle)', 'Avg edges between communities (middle)')
kable(df[1:8,], format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F,
      caption = 'Summary statistics for all small world networks in the complex networks datset')%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')%>%kable_styling(font_size = 12)
}



```


\newpage
```{r tab2, eval = T, echo=F, message=F, warning=F}


if(compile.to.pdf){
  kable(t(df[c(1, 10:17),]), format = 'latex', digits = 2, row.names = F, col.names = c('Value', paste0('Network', c(1:8))),
                                                                                  booktabs = T,
     caption = 'Summary statistics for all scale free networks in the complex networks datset')%>%kable_styling(latex_options = c("striped", "hold_postion"),font_size = 10)%>%kable_styling(font_size = 9)%>%column_spec(1, '6em')%>%column_spec(2:9, '4em')
}else{
  kable(df[c(9:16),], format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F,
      caption = 'Summary statistics for all scale free networks in the complex networks datset')%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')%>%kable_styling(font_size = 12)
}




```


\newpage
```{r tab3, eval = T, echo=F, message=F, warning=F}


if(compile.to.pdf){
  kable(t(df[c(1, 18:25),]), format = 'latex', digits = 2, row.names = F, col.names = c('Value', paste0('Network', c(1:8))),
                                                                                    booktabs = T,
      caption = 'Summary statistics for all random graph networks in the complex networks datset')%>%kable_styling(latex_options = c("striped", "hold_postion"),font_size = 10)%>%kable_styling(font_size = 9)%>%column_spec(1, '6em')%>%column_spec(2:9, '4em')
}else{
  kable(df[c(17:24),], format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F,
      caption = 'Summary statistics for all random graph networks in the complex networks datset')%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')%>%kable_styling(font_size = 12)
}


```


\newpage
```{r tab4, eval = T, echo=F, message=F, warning=F, fig.cap = 'This table gives a summary of the hierarchical networks in the intermediate networks dataset where origin nodes are initialized from a normal distribution with unique means for each community.'}

inter_net_stats = read.csv('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/Intermediate_examples_unique_dist/intermediate_examples_network_statistics.csv')[,-1]

inter_net_stats$modularity_top = round(inter_net_stats$modularity_top, 3)
inter_net_stats$avg_node_degree_top = round(inter_net_stats$avg_node_degree_top, 3)
inter_net_stats$avg_connect_within_top = round(inter_net_stats$avg_connect_within_top, 3)
inter_net_stats$avg_connect_between_top = round(inter_net_stats$avg_connect_between_top, 3)
inter_net_stats$modularity_middle = round(inter_net_stats$modularity_middle, 3)
inter_net_stats$avg_node_degree_middle = round(inter_net_stats$avg_node_degree_middle, 3)
inter_net_stats$avg_connect_within_middle = round(inter_net_stats$avg_connect_within_middle, 3)
inter_net_stats$avg_connect_between_middle = round(inter_net_stats$avg_connect_between_middle, 3)





if(compile.to.pdf){
  df = rbind.data.frame(c('Subgraph type','Connect. type','Layers','StDev.','Nodes per layer','Edges per layer','Subgraph prob.','Sample size', 'Modularity (top)', 'Avg. node degree top', 'Avg edges within communities (top)','Avg. edges between communities (top)', 'Modularity (middle)', 'Avg. node degree middle', 'Avg edges within communities (middle)', 'Avg edges between communities (middle)'), inter_net_stats)
  kable(t(df), format = 'latex', digits = 2, row.names = F, col.names = c('Value', paste0('Network', c(1:6))),
                                                                                    booktabs = T,
      caption = 'Summary statistics for intermediate difficulty simulated networks.')%>%kable_styling(latex_options = c("striped", "hold_postion"),font_size = 10)%>%kable_styling(font_size = 10)%>%column_spec(1, '8em')
}else{
    df = inter_net_stats
    colnames(df) = c('Subgraph type','Connect. type','Layers','StDev.','Nodes per layer','Edges per layer','Subgraph prob.','Sample size', 'Modularity (top)', 'Avg. node degree top', 'Avg edges within communities (top)','Avg. edges between communities (top)', 'Modularity (middle)', 'Avg. node degree middle', 'Avg edges within communities (middle)', 'Avg edges between communities (middle)')
  kable(df, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F,
      caption = 'Summary statistics for intermediate difficulty simulated networks.')%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')%>%kable_styling(font_size = 12)
}




```


\newpage
```{r tab5, eval = T, echo=F, message=F, warning=F, fig.cap = 'This table gives a summary of the networks included in the simple networks dataset'}

toy_net_stats = read.csv('C:/Users/Bruin/Documents/GitHub/HGRN_repo/Simulated Hierarchies/DATA/Toy_examples/toy_examples_network_statistics.csv')[,-1]

toy_net_stats$modularity_top = round(toy_net_stats$modularity_top, 3)
toy_net_stats$avg_node_degree_top = round(toy_net_stats$avg_node_degree_top, 3)
toy_net_stats$avg_connect_within_top = round(toy_net_stats$avg_connect_within_top, 3)
toy_net_stats$avg_connect_between_top = round(toy_net_stats$avg_connect_between_top, 3)
toy_net_stats$modularity_middle = round(toy_net_stats$modularity_middle, 3)
toy_net_stats$avg_node_degree_middle = round(toy_net_stats$avg_node_degree_middle, 3)
toy_net_stats$avg_connect_within_middle = round(toy_net_stats$avg_connect_within_middle, 3)
toy_net_stats$avg_connect_between_middle = round(toy_net_stats$avg_connect_between_middle, 3)




if(compile.to.pdf){
  
  df = rbind.data.frame(c('Subgraph type','Connect. type','Layers','StDev.','Nodes per layer','Edges per layer','Subgraph prob.','Sample size', 'Modularity (top)', 'Avg. node degree top', 'Avg edges within communities (top)','Avg. edges between communities (top)', 'Modularity (middle)', 'Avg. node degree middle', 'Avg edges within communities (middle)', 'Avg edges between communities (middle)'), toy_net_stats)
  kable(t(df), format = 'latex', digits = 2, row.names = F, col.names = c('Value', paste0('Network', c(1:4))),
                                                                                    booktabs = T,
      caption = 'Summary statistics for simple simulated networks. These networks contain fewer than 100 nodes at the observed level and only cover small world subgraph architecture')%>%kable_styling(latex_options = c("striped", "hold_postion"),font_size = 10)%>%kable_styling(font_size = 10)%>%column_spec(1, '8em')
}else{
  
  df = toy_net_stats
  colnames(df) = c('Subgraph type','Connect. type','Layers','StDev.','Nodes per layer','Edges per layer','Subgraph prob.','Sample size', 'Modularity (top)', 'Avg. node degree top', 'Avg edges within communities (top)','Avg. edges between communities (top)', 'Modularity (middle)', 'Avg. node degree middle', 'Avg edges within communities (middle)', 'Avg edges between communities (middle)')
  kable(df, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F,
      caption = 'Summary statistics for simple simulated networks. These networks contain fewer than 100 nodes at the observed level and only cover small world subgraph architecture.')%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')%>%kable_styling(font_size = 12)
}



```


\newpage
```{r tab6, echo=F, eval = F, message=F, warning=F}

grid = expand.grid(`Input Graph` = unique(tab$input_graph),
                   `Graph Recon. Loss` = c('1 = on', '0 = off'),
                   `Attr. Recon. Loss` = c('False (on)', 'True (off)'),
                   `Modularity Weight` = c('1 = on', '0 = off'),
                   `Clust. Weight` = c('1 (middle), 1 (top)', '0.1 (middle), 1e-4 (top)'))


if(compile.to.pdf){
  kable(grid, format = 'latex', digits = 2, row.names = F, booktabs = T,
      caption = 'Simulation settings for intermediate difficulty networks. Each row represents a single simulation scenario applied to all 6 networks in the intermediate networks dataset.')%>%kable_styling(latex_options = c("striped", "hold_postion"),font_size = 10)%>%kable_styling(font_size = 10)%>%column_spec(1, '8em')
}else{
  kable(grid, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F,
      caption = 'Simulation settings for intermediate difficulty networks. Each row represents a single simulation scenario applied to all 6 networks in the intermediate networks dataset.')%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')%>%kable_styling(font_size = 12)
}



```






```{r example121314fig1, eval = T, echo =F,  fig.cap='True and predicted adjacency matrices for examples 12 - 14', fig.height=8, fig.width=12}
include_graphics(paste0(figpath, 'Fu HCD Presentation Files/Adjacency_Heatmaps_combined.png'))

```

```{r example121314fig2, eval = T, echo =F,  fig.cap='Heatmaps showing HCD prediction performance for examples 12 - 14', fig.height=8, fig.width=12}
include_graphics(paste0(figpath, 'Fu HCD Presentation Files/heatmaps.png'))

```

```{r example121314fig3, eval = T, echo =F,  fig.cap='Correlations on the simulated data and HCD model bottleneck dimension embeddings for examples 12-14', fig.height=8, fig.width=12}
include_graphics(paste0(figpath, 'Fu HCD Presentation Files/Correlation_graphs_combined.png'))

```


























# References
\bibliographystyle{unsrt}
	\bibliography{C:/Users/Bruin/Desktop/Research Assistantship/Thesis Proposal Defense/proposal_references.bib}

\newpage
\section*{References}
