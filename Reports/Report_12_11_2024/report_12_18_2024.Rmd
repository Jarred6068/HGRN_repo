---
title: "HCD Simulations Write Up"
author: "Audrey Fu Lab"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
always_allow_html: true
bibliography: C:/Users/Bruin/OneDrive/Desktop/Research Assistantship/Thesis Proposal
  Defense/proposal_references.bib
number_sections: false
---

```{r setup, include=FALSE}

library(ggplot2)
library(ggthemes)
library(ggpubr)
library(latex2exp)
library(gridExtra)
library(knitr)
library(kableExtra)
library(dplyr)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8)

```


# Data Simulation {-}

### Simulating networks {-}
We adopt a top-down approach to simulate hierarchical networks, considering various simulation parameters such as graph sparsity, noise, and the architecture of the super-level graph(s), including small-world, scale-free, and random graph networks [@watts1998collective; @barabasi2003scale].

Our simulations focus on basic hierarchies comprising one or two hierarchical layers. Two-layer networks mirror classical community detection on graphs, where our aim is to recover the true community labels from a given graph. Meanwhile, three-layer networks present a more intricate scenario, where the bottom layer of the hierarchy contains two levels of community structure. Here, the top level corresponds to the nodes at the uppermost layer of the hierarchy, and the middle level consists of communities nested within the top-level communities. The objective with these networks is to identify both sets of community partitions.
	
In each hierarchy, for fully connected networks, we initiate by simulating $N_{\text{top}}$ top-level nodes, adhering to a directed small-world, random graph, or scale-free network architecture [@watts1998collective; @barabasi2003scale]. In cases where the network is disconnected, we simply simulate $N_{\text{top}}$ disconnected nodes. For networks with three hierarchical layers, we then generate a subnetwork of $N_{\text{middle}}$ nodes from each top-layer node, adhering to the network structure utilized at the top level. If the network is fully connected, we apply a probability $p_\text{between}$ to the nodes from different top-level communities being connected. 

The final step in all hierarchies is to generate the nodes in the observed (bottom) layer of the hierarchy. For each top-layer or middle-layer node, we generate a sub-network of $N_{\text{bottom}}$ nodes under the same sub-network structure as the previous layers, and we apply a probability $p_\text{between}$ for nodes from different communities to share an edge.


### Simulating gene expression {-}

Once we simulate a hierarchical graph, we utilize this hierarchy to generate the node-feature matrix, which depicts the expression of $N$ genes across $d$ samples. Here, $N$ denotes the number of nodes in the observed (bottom) layer of the hierarchy.

We simulate the node-feature matrix using the topological order of the observed level graph. We start by generating the features of nodes that have no parental input. We refer to these nodes as *origin* nodes. All origin nodes are simulated from a normal distribution with mean $0$ and standard deviation $\sigma$. All other nodes are simulated from a normal distribution centered at the mean of their parent nodes and with standard deviation $\sigma$. 

# Hierarchical Commuity Detection (HCD) Overview {-}

Our HCD method consists of two modules:

  1. A graph autoencoder based on the architecture proposed by @salehi2019graph which utilizes graph attention layers such as those first introduced by @velivckovic2017graph (See most recent version of pseudocode for details). In our applications, we incorporate multi-head attention in all encoder and decoder layers to expand model learning capacity. The graph autoencoder module takes a set of node attributes ${\bf X}\in\mathbb{R}^{N \times d}$ and an adjacency matrix ${\bf A}_{ij}\in[0,1]$ defining the relationships between the node as input and learns a low dimensional embedding ${\bf Z} \in \mathbb{R}^{N \times q}$ of the network and attributes. This embedding is then used to reconstruct the node attributes and adjacency matrix under a separate loss function for each.  
  
  2. The second module of HCD takes the embeddings ${\bf Z}$ generated by the autoencoder and applies a multilevel community detection process. This module first applies the function $f_{top}$ which partitions the data into $k$ groups. The function $f_{top}$ can be either (i) the $SoftKMeans$ layer described by [@Falkner2022] or a neural layer such as 
  
  $$f_{top}({\bf Z}) = \theta({\bf ZW} + {\bf b})$$
  The above example is a simple fully connected layer where ${\bf W} \in \mathbb{R}^{N \times k}$ is a matrix of parameters, ${\bf b} \in \mathbb{R}^N$ is a bias parameter vector and $\theta$ is an activation function such as the $ReLU$ function. Alternative types of layer can also be used such the GraphSAGE convolution described by [@] or graph attention. 
  
# Simulations

Here we describe the settings for three different sets of simulations. For each set of simulations, we simulated hierarchical gene networks consisting of 5 top level nodes/communities, 15 middle level nodes/communities and 300 nodes/genes at the observed level of the hierarchy. We simulate networks for all three graph types (small world, scale free or random graph) with the nodes at the top level of the hierarchy either all connected or all disconnected. We also simulated nodes under two levels of noise with noise set to either $\sigma = 0.1$ or $\sigma = 0.5$. Additionally, we sample the probability for edges within and between node communities at the middle layer and bottom level from the uniform distributions


```{r simtab1, echo = FALSE, message=FALSE, warning=FALSE}

x = cbind.data.frame(Layer = c("Middle", "", "Bottom", ""),
                     Connection = c("Within", "Between", "Within", "Between"),
                     `Sampling Distibution` = rep('Uniform', 4),
                     `Lower Bound` = c(0.01, 0.01, 0.001, 0.001),
                     `Upper Bound` = c(0.15, 0.15, 0.05, 0.05))
kable(x, format = 'latex', booktabs = T)

```

Each combination of parameters (graph type, connectivity, and noise) is replicated multiple times. 


# Set 1 Simulations

In this set of simulations, the HCD model uses a linear neural layer for both the top and middle layer partitions. For these experiments, we set the output size of each neural network to match the ground truth (i.e., output size of 5 for the top layer and 15 for the middle layer). Unlike classical hierarchical clustering, where setting $k$ identifies the "best" cut of the dendrogram to find $k$ communities, setting the output size to the ground truth for the neural network predictors does not guarantee they will predict exactly 5 or 15 communities. Instead, it sets an upper bound on the number of communities that may be found. A comprehensive list of parameter settings is outlined in the table below.


```{r paramtab1, echo = FALSE, message=FALSE, warning=FALSE}

params <- data.frame(
  Parameter = c("patience", "method", "batch learning", "batch_size", "AE operator", 
                "COMM operator", "attn. heads", "dropout", "Layer Normalization", 
                "AE hidden sizes", "Comm output size (Top)", "Comm output size (Middle)", 
                "$\\gamma$", "$\\delta$", "$\\lambda$", "learning rate", "Max epochs", 
                "Graph correlation cutoff", "% Training", "% Testing"),
  
  Value = c(10, "top down", "TRUE", 64, "GATv2Conv","Linear", 5, 0.2, "TRUE", "[256, 128]", 
            5, 15, 1, 1, "[1, 1]", 1e-3, 200, 0.2, 80, 20)
)

# Create kable table
kable(params,
      format = "latex",
      col.names = c("Parameter", "Value"),
      caption = "Simulation Set 1 model parameter settings") %>%
  kable_styling(full_width = FALSE)

```


# Set 2 Simulations

In Set 2 of our simulations, we apply HCD using the same model and network simulation settings as Set 1, which are outlined in **Tables** \@ref(tab:paramtab1) - \@ref(tab:simtab1). However, in this scenario, we investigate HCD's performance on complex networks with approximately 1,000 nodes in the hierarchy's bottom layer.

```{r, echo = FALSE, message=FALSE, warning=FALSE}

params <- data.frame(
  Parameter = c("patience", "method", "batch learning", "batch_size", "AE operator", 
                "COMM operator", "attn. heads", "dropout", "Layer Normalization", 
                "AE hidden sizes", "Comm output size (Top)", "Comm output size (Middle)", 
                "$\\gamma$", "$\\delta$", "$\\lambda$", "learning rate", "Max epochs", 
                "Graph correlation cutoff", "% Training", "% Testing"),
  
  Value = c(10, "top down", "TRUE", 64, "GATv2Conv","Linear", 5, 0.2, "TRUE", "[256, 128]", 
            5, 15, 1, 1, "[1, 1]", 1e-3, 200, 0.2, 80, 20)
)

# Create kable table
kable(params,
      format = "pipe",
      col.names = c("Parameter", "Value"),
      caption = "Model Parameter Settings") %>%
  kable_styling(full_width = FALSE)

```

# Set 3 Simulations

In Set 3, we implement HCD using SoftKmeans for the top-layer partitioning, while maintaining all other parameters identical to those in Set 1.

# Set 4 Simulations

In Set 4, we implement HCD following the parameters of Set 1, but adopt an unsupervised approach. We use the Beth Hessian to estimate the number of communities in the top layer, while fixing the middle layer's output size to 64 communities.


The **Beth Hessian Matrix** is a spectral approach derived from the Hessian matrix of the Bethe free energy in statistical physics, adapted for community detection in networks. It provides a robust spectral approach to estimate the number of communities in a graph.

$$ $$

where 


### Evaluating performance {-}

We evaluate the performance of our HCD method using three graph-based clustering metrics:

   1. **homogeneity** evaluates the degree to which each predicted community contains only data points from a single true community, indicating how well the algorithm avoids mixing different groups. Thus, homogeneity tends to be high if resolved communities contain only members of the same true community.
   
   2. **completeness** assesses the extent to which all data points that belong to the same true
community are correctly assigned to a single predicted community. Thus completeness is always high if all members of the same true communities end up in the same resolved community even if several true communities are allocated together. 
   
   3. **NMI** Normalized Mutual Information (NMI) is a weighted average of the Completeness and Homogeneity two metrics. 
   
   4. **ARI** The Adjusted Rand Index (ARI) is a metric that measures the similarity between two different clusterings of the same data, correcting for the chance of random agreement. It ranges from -1 to 1, where 1 indicates perfect agreement between the clusterings, 0 represents random labeling, and negative values indicate less agreement than expected by chance. 


In all simulations we compare HCD with two commonly used heuristic methods:

  * **Louvain Method**: This widely used community detection algorithm optimizes modularity by iteratively reassigning nodes between communities and merging communities. It automatically determines the number of communities and is highly efficient for processing large networks. In all simulations, we apply the Louvain method to the estimated adjacency matrix, which is derived from the correlation matrix of the node features. The resulting community predictions are compared to the ground truth for both the top and middle layers of the simulated hierarchy.

  * **Hierarchical Clustering using Ward's Distance (HC)**: This agglomerative clustering approach iteratively merges clusters to minimize the increase in within-cluster variance. Ward's method tends to produce compact, spherical clusters and generates a dendrogram representing the full hierarchical structure of the data. In all simulations, HC is applied to the simulated gene expression data (node feature matrix). The optimal community predictions are determined by identifying the best cutting point on the dendrogram, aligning with the ground truth clusters (five clusters and 15 clusters).



 



  
### Points to investigate











```{r, echo=F, message=F, warning=F}

fp = 'C:/Users/Bruin/OneDrive/Documents/GitHub/HGRN_repo/Reports/Report_1_3_2025/Output/Intermediate_applications/SET1/linear_layers_1_9_2025/'
graphs = c('small_world', 'scale_free', 'random_graph')
connect = c('disc', 'full')
case = 'Case'
sd = c('01', '05')
case.num = as.character(c(0:24))

fns = expand.grid(graphs, connect, sd, case, case.num)

fn = apply(fns, MARGIN = 1, FUN = function(x) paste0(paste(x[1:2], collapse = '_'), paste(x[3:5], collapse = '_')))


tables_mid = vector('list', length = length(fn))
tables_top = vector('list', length = length(fn))
for(i in 1:length(fn)){
  
  tables_mid[[i]] = read.csv(paste0(fp, fn[i], '.csv'), row.names = 1)[c(1,3,6),]
  tables_top[[i]] = read.csv(paste0(fp, fn[i], '.csv'), row.names = 1)[c(2,4,5),]
  
}

data_mid = do.call('rbind', tables_mid)
data_top = do.call('rbind', tables_top)


data_top$subgraph_type = as.factor(data_top$subgraph_type)
data_top$Method = as.factor(data_top$Method)
data_top$Method <- recode(data_top$Method,
                          "Louvain Top" = "Louvain",
                          "HCD Top" = "HCD",
                          "HC Top" = "HC" )



data_mid$subgraph_type = as.factor(data_mid$subgraph_type)
data_mid$Method = as.factor(data_mid$Method)
data_mid$Method <- recode(data_mid$Method,
                          "Louvain Middle" = "Louvain",
                          "HCD Middle" = "HCD",
                          "HC Middle" = "HC" )

data_top$StDev = as.factor(data_top$StDev)
data_mid$StDev = as.factor(data_mid$StDev)
# change names 


```



```{r, echo=F, message=F, warning=F, fig.cap="Hierarchical clustering performance for the top layer of the hierarchical for HCD, classical hierarchical clustering using Ward's Distance (HC) and the Louvain method. Each point represents the performance in homogeneity and completeness for a unique simulated hierarchy. The diagonal line represents 1 to 1 corresponds between completeness and homogeneity. Points that fall above the line represent clustering solutions that overgroup the true communities while points that fall below the line represent clusters that break up the true communities. Points on the diagonal line represent a better clustering solution"}


A = ggplot(data = data_top, aes(x = Homogeneity, y = Completeness, color = Method, shape = connection_type))+
  geom_abline(slope = 1, intercept = 0, linewidth = 1)+
  geom_point(size = 2)+
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1))+
  facet_wrap(~subgraph_type)+
  theme_bw()+
  ggtitle('Top Layer Clustering Performance')+
  theme(axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title = element_text(size = 16),
        legend.position = 'top', 
        legend.text = element_text(size = 10))

plot(A)

```


```{r, echo=F, message=F, warning=F, fig.cap = "Hierarchical clustering performance for the middle layer of the hierarchical for HCD, classical hierarchical clustering using Ward's Distance (HC) and the Louvain method. Each point represents the performance in homogeneity and completeness for a unique simulated hierarchy. The diagonal line represents 1 to 1 corresponds between completeness and homogeneity. Points that fall above the line represent clustering solutions that overgroup the true communities while points that fall below the line represent clusters that break up the true communities. Points on the diagonal line represent a better clustering solution."}

B = ggplot(data = data_mid, aes(x = Homogeneity, y = Completeness, color = Method, shape = connection_type))+
  geom_abline(slope = 1, intercept = 0, linewidth = 1)+
  geom_point(size = 2, alpha = 0.8)+
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1))+
  facet_wrap(~subgraph_type)+
  theme_bw()+
  ggtitle('Middle Layer Clustering Performance')+
  theme(axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title = element_text(size = 16),
        legend.position = 'top', 
        legend.text = element_text(size = 10))

plot(B)

```





# complex networks 


```{r, echo=FALSE, message=FALSE, warning=FALSE}

#fp = 'C:/Users/Bruin/OneDrive/Documents/GitHub/HGRN_repo/Reports/Report_12_11_2024/Output/Intermediate_applications/downweighted_linear/'
fp = 'C:/Users/Bruin/OneDrive/Documents/GitHub/HGRN_repo/Reports/Report_12_11_2024/Output/complex_applications/'
graphs = c('small_world', 'scale_free', 'random_graph')
connect = c('disc', 'full')
case = 'Case'
sd = c('0.1', '0.5')
#case.num = as.character(c(0:49))
case.num = as.character(c(0:24))

#random_graph_disc_0.1__Case__0

fns = expand.grid(graphs, connect, sd, case, case.num)

fn = apply(fns, MARGIN = 1, FUN = function(x) paste(c(paste(x[1:3], collapse = '_'), x[4], x[5]), collapse = '__'))


tables_mid = vector('list', length = length(fn))
tables_top = vector('list', length = length(fn))
for(i in 1:length(fn)){
  
  tables_mid[[i]] = read.csv(paste0(fp, fn[i], '.csv'), row.names = 1)[c(1,3,6),]
  tables_top[[i]] = read.csv(paste0(fp, fn[i], '.csv'), row.names = 1)[c(2,4,5),]
  
}

data_mid = do.call('rbind', tables_mid)
data_top = do.call('rbind', tables_top)


data_top$subgraph_type = as.factor(data_top$subgraph_type)
data_top$Method = as.factor(data_top$Method)
data_top$Method <- recode(data_top$Method,
                          "Louvain Top" = "Louvain",
                          "HCD Middle" = "HCD",
                          "HC Top" = "HC" )



data_mid$subgraph_type = as.factor(data_mid$subgraph_type)
data_mid$Method = as.factor(data_mid$Method)
data_mid$Method <- recode(data_mid$Method,
                          "Louvain Middle" = "Louvain",
                          "HCD Top" = "HCD",
                          "HC Middle" = "HC" )

data_top$StDev = as.factor(data_top$StDev)
data_mid$StDev = as.factor(data_mid$StDev)
# change names 


```



```{r, echo=F, message=F, warning=F, fig.cap="Hierarchical clustering performance for the top layer of the hierarchical for HCD, classical hierarchical clustering using Ward's Distance (HC) and the Louvain method. Each point represents the performance in homogeneity and completeness for a unique simulated hierarchy. The diagonal line represents 1 to 1 corresponds between completeness and homogeneity. Points that fall above the line represent clustering solutions that overgroup the true communities while points that fall below the line represent clusters that break up the true communities. Points on the diagonal line represent a better clustering solution"}


A = ggplot(data = data_top, aes(x = Homogeneity, y = Completeness, color = Method, shape = connection_type))+
  geom_abline(slope = 1, intercept = 0, linewidth = 1)+
  geom_point(size = 2)+
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1))+
  facet_wrap(~subgraph_type)+
  theme_bw()+
  ggtitle('Top Layer Clustering Performance')+
  theme(axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title = element_text(size = 16),
        legend.position = 'top', 
        legend.text = element_text(size = 10))

plot(A)


```


```{r, echo=F, message=F, warning=F, fig.cap = "Hierarchical clustering performance for the middle layer of the hierarchical for HCD, classical hierarchical clustering using Ward's Distance (HC) and the Louvain method. Each point represents the performance in homogeneity and completeness for a unique simulated hierarchy. The diagonal line represents 1 to 1 corresponds between completeness and homogeneity. Points that fall above the line represent clustering solutions that overgroup the true communities while points that fall below the line represent clusters that break up the true communities. Points on the diagonal line represent a better clustering solution."}

B = ggplot(data = data_mid, aes(x = Homogeneity, y = Completeness, color = Method, shape = connection_type))+
  geom_abline(slope = 1, intercept = 0, linewidth = 1)+
  geom_point(size = 2)+
  scale_x_continuous(limits = c(0,1))+
  scale_y_continuous(limits = c(0,1))+
  facet_wrap(~subgraph_type)+
  theme_bw()+
  ggtitle('Middle Layer Clustering Performance')+
  theme(axis.text.x = element_text(size = 14),
        axis.text.y = element_text(size = 14),
        axis.title = element_text(size = 16),
        legend.position = 'top', 
        legend.text = element_text(size = 10))

plot(B)

```




